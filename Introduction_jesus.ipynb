{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea E-3. \n",
    "\n",
    "**Autores**: Jesús Martínez Leal y Carlos Sánchez Polo\n",
    "\n",
    "**Enunciado**:\n",
    "\n",
    "El algoritmo DOMINANT de `detección de anomalías` en grafos está basado en las ideas de redes convolucionales (GNN) y autoencoders (GAE) de los grafos.\n",
    "\n",
    "- Introduce teóricamente las redes convolucionales en grafos (GNN).\n",
    "- Aplica las GNN en ejemplos sencillos en los que se muestren los conceptos anteriormente introducidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos previos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción a los grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los grafos son estructuras matemáticas que nos permiten representar y modelar relaciones entre diversas entidades. En su forma más básica, un grafo está compuesto por un conjunto de `nodos` (también llamados vértices) unidos por enlaces llamados `aristas` (también llamados bordes). Típicamente, un grafo se representa como diagrama como un conjunto de círculos para los vértices, unido por líneas o curvas para las aristas. \n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/graph-theory-in-discrete-mathematics.png\" alt=\"Figura de un grafo\">\n",
    "  <figcaption>Figura 1: Representación de un grafo.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Los enlaces pueden ser `no dirigidos` o `dirigidos`, lo que significa que pueden representar relaciones unidireccionales o bidireccionales entre los nodos, respectivamente. Ejemplos de esto serían:\n",
    "\n",
    "- **Enlaces no dirigidos**. Imaginemos que tenemos un grafo con vértices representando a personas en una fiesta y existe una conexión si dos personas se dan la mano. Las conexiones serán efectivamente no dirigidas dada la reprocidad que implica darse la mano.\n",
    "\n",
    "- **Enlaces dirigidos**. Imaginemos ahora que el grafo cuenta con vértices representando las conexiones entre distintas estaciones de tren. Podría darse el caso que la estación A estuviera conectada con la estación B directamente, pero que no fuera al contrario.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <figure style=\"display:inline-block;\">\n",
    "    <img src=\"./data/conexion_no_dirigida.png\" alt=\"Conexión no dirigida\">\n",
    "    <figcaption>Figura 2: Enlace no dirigido.</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display:inline-block;\">\n",
    "    <img src=\"./data/conexion_dirigida.png\" alt=\"Conexión dirigida\">\n",
    "    <figcaption>Figura 3: Enlace dirigido.</figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "Formalmente, un grafo $G$ es un `par ordenado` $G = (V, E)$, donde $V$ es un conjunto de nodos y $E$ un conjunto de aristas que relacionan estos. En un `grafo no dirigido` encontramos que todos los enlaces son no dirigidos, sucediendo justamente lo opuesto en un `grafo dirigido`. Por otra parte,  un `grafo mixto` permite los dos tipos de enlaces en su estructura, siendo un caso más general de los anteriores.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cabe mencionar que los elementos individuales de cada grafo pueden almacenar información adicional:\n",
    "\n",
    "- Información en nodos. La información almacenada en cada nodo puede incluir atributos para caracterizar mejor la entidad que representa.\n",
    "- Información en enlaces. La información aquí puede incluir atributos que describen la naturaleza de la relación entre los nodos. Puede incluir pesos para la ponderar la fuerza, distancia o similitud entre los nodos conectados (`grafo ponderado`).\n",
    "- Información en el grafo en su conjunto. Información como el número total de nodos y aristas, densidad del grafo, etc.\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/information_graphs.png\" alt=\"Información incrustada en un grafo\">\n",
    "  <figcaption>Figura 4: Información incrustada dentro de los elementos de un grafo.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/weighted_graph2.png\" alt=\"Grafo ponderado\">\n",
    "  <figcaption>Figura 5: Grafo ponderado.</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los grafos constituyen herramientas poderosas y versátiles que se utilizan en amplia variedad de campos, debido a su capacidad de modelar y representar una amplia gama de relaciones y estructuras complejas.\n",
    "\n",
    "Puede resultar sorprendente, pero ¡los grafos son capaces de modelar objetos tan comunes como `imágenes` y `texto`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción a las redes neuronales (convolucionales se detalla algo más)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las `redes neuronales` son modelos computacionales inspirados en el cerebro humano que se utilizan para aprender y realizar tareas específicas. Han tenido un impacto muy significativo en la historia de la humanidad, permitiendo mejoras drásticas en muchas áreas de tecnología, medicina, industria, investigación científica...\n",
    "\n",
    "Consisten en un conjunto de unidades llamadas `neuronas` conectadas entre sí. La información de entrada atraviesa la red sometiéndose a diversas operaciones, produciendo valores de salida.\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/neural_network_basic.png\" alt=\"Red neuronal básica\">\n",
    "  <figcaption>Figura 5: Esquema básico de una red neuronal.</figcaption>\n",
    "</figure>\n",
    "\n",
    "La fuerza de conexión entre las distintas neuronas se ve modelada por los `pesos` de la red, parámetros del modelo que se van ajustando durante el `proceso de entrenamiento`.\n",
    "\n",
    "Típicamente, las neuronas están distribuidas en capas, permitiendo una distribución de la tarea a ejecutar en un problema determinado para el objetivo a buscar. Se tienen la `capa de entrada`, `capas ocultas` y la `capa de salida`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCIONAMIENTO BÁSICO**\n",
    "\n",
    "Existen muchos tipos diversos de redes neuronales y de particularidades, pero a continuación se describe la base del entrenamiento de una red neuronal.\n",
    "\n",
    "- `Paso hacia delante` (Forward Pass).\n",
    "\n",
    "Durante esta etapa los datos se propagan hacia delante. El recorrido empieza desde la capa de entrada y continúa por las capas ocultas, donde se sigue la siguiente relación para determinar cómo se transmite la información entre neuronas de una capa anterior a una posterior.\n",
    "\n",
    "$$x_{j}^{l + 1} = f (\\sum_{i} w_{ij}^l x_{i} ^l)$$ \n",
    "\n",
    "donde $l$ designa la capa en cuestión, $j$ es el índice que va sobre las neuronas de la capa $l + 1$ y $i$ el que va sobre las neuronas de la capa $l$: $w_{ij}^l$ es pues el peso que conecta la neurona i-ésima de la capa $l$ con la neurona j-ésima de la capa $l + 1$. Por otra parte, $f$ es una función de `activación` generalmente no lineal, permitiendo enriquecer la captura de estructuras en los datos por parte del modelo.\n",
    "\n",
    "La capa de salida finalmente permite sacar del modelo los valores finales que nos interesen en nuestra tarea.\n",
    "\n",
    "- `Función de pérdida`.\n",
    "\n",
    "Después del paso hacia delante, la salida de la red se compara con los valores verdaderos (esto es si es problema supervisado) usando una función de pérdida (`loss function`), dependiente del problema que queremos tratar.\n",
    "\n",
    "- `Paso hacia atrás` (Backward pass).\n",
    "\n",
    "Durante esta etapa la red va actualizando sus pesos tratando de minimizar la función de pérdida. Para ello, se calculan primeramente los gradientes de la función de pérdida respecto a los pesos de la red, haciendo uso de la *regla de la cadena* que todos aprendimos una vez en cálculo. Posteriormente, se ajustan los pesos en la dirección que se minimice dicha función.\n",
    "\n",
    "Todo esto se realiza de manera iterativa, siguiendo las instrucciones dadas por el usuario (`hiperparámetros`, `callbacks`, `algoritmos de optimización`, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIPOS BÁSICOS DE REDES NEURONALES**\n",
    "\n",
    "Hay una cantidad muy grande de tipos de redes neuronales, pero aquí se muestra la idea básica de los más utilizados.\n",
    "\n",
    "1. `Redes neuronales de avance (o de propagación hacia delante).`\n",
    "\n",
    "Las conexiones entre neuronas **no** forman un ciclo en este tipo. La información discurre desde la capa de entrada hasta la capa de salida.\n",
    "\n",
    "- Perceptrón simple (una sola capa).\n",
    "\n",
    "Este es el modelo más simple de red neuronal de avance. No contiene una capa oculta.\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/perceptron_simple.gif\" alt=\"Perceptrón simple\">\n",
    "  <figcaption>Figura 6: Esquema de un perceptrón simple.</figcaption>\n",
    "</figure>\n",
    "\n",
    "- Perceptrón multicapa (MLP).\n",
    "\n",
    "Este es el modelo más común a la hora de introducir las redes neuronales: básicamente tenemos los distintos tipos de capas vistas anteriormente.\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/mlp_image.png\" alt=\"Perceptrón multicapa\">\n",
    "  <figcaption>Figura 7: Esquema de un perceptrón multicapa sencillo.</figcaption>\n",
    "</figure>\n",
    "\n",
    "- Redes neuronales convolucionales (CNN).\n",
    "\n",
    "Este tipo de redes son especialmente conocidas por su rendimiento excelente para extraer características en audio y (sobre todo) de imágenes. Esto enfoca la arquitectura para configurarse de tal manera que se adapte mejor a la necesidad de tratar con este tipo de datos.\n",
    "\n",
    "Las neuronas en las capas dentro de las CNN están organizadas en tres dimensiones: la dimensionalidad espacial de la entrada (altura y anchura) y la profundidad. Esta profundidad no tiene que ver con el número total de capas dentro de la red, sino a la tercera dimensión de un `volumen de activación`. \n",
    "\n",
    "Poseen tres tipos de capas distinguidas: las `capas convolucionales`, las capas `pooling` (para submuestreo) y las capas `fully-connected` (completamente conectadas).\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/cnn_architecture.png\" alt=\"Arquitectura CNN simple\">\n",
    "  <figcaption>Figura 8: Arquitectura de una red convolucional simple.</figcaption>\n",
    "</figure>\n",
    "\n",
    "`Capas convolucionales`: van a determinar la salida de neuronas que están conectadas a regiones locales de la entrada a través del cálculo del producto escalar entre sus pesos y la región conectada al volumen de entrada. En otras palabras, estas capas aplican filtros o `kernels` (se aprenden solos) a la entrada para detectar características relevantes de los datos, como bordes, texturas o patrones específicos. Cada neurona en esta capa está conectada pues solo a una pequeña región de la imagen de entrada, lo que permite que la red capture características locales y las comparta en toda la imagen (todo esto asociado a tener un número mucho menor de parámetros que los que habría en una capa completamente conectada). \n",
    "\n",
    "Concretamente, el kernel se aplica a un área de la imagen y se calcula un producto escalar entre los píxeles de entrada y el filtro. Se produce un mapa de activación (o de características), en el que básicamente se resaltan las áreas donde se encuentran las características interesantes que el kernel buscaba. Este puede visualizarse posteriormente.\n",
    "\n",
    "Hay 3 hiperparámetros muy importantes que afectarán al tamaño de salida:\n",
    "\n",
    "- **Número de filtros**: afecta a la profundidad de salida. Por ejemplo, tres filtros distintos darían tres mapas de características diferentes.\n",
    "- **Stride (paso, en español)**: es el número de píxeles que el kernel se mueve sobre la matriz de input. Un stride mayor, da un output menor.\n",
    "- **Zero-padding**: usualmente se utiliza cuando los filtros no \"caben\" en la imagen de entrada. Esto hace que todos los elementos que caen fuera se pongan como 0. Están los tipos `valid`, `same` y `full`.\n",
    "\n",
    "Después de cada operación de convolución, se suele aplicar una transformación ReLU al mapa de activación, introduciendo no linealidad.\n",
    "\n",
    "`Capas de pooling`: hacen submuestreo para la dimensionalidad espacial de una entrada, reduciendo así el número de parámetros. De manera similar a la capa convolucional, se tiene a un filtro barriendo toda la entrada, pero la diferencia es que este filtro no tiene pesos. En su lugar, el kernel aplica una función de agregación a los valores dentro del `campo receptivo` (trozo que ve cada neurona). Están los tipos `max` (donde se coge el máximo) y `average` (donde se toma el promedio).\n",
    "\n",
    "`Capas fully-connected`: tienen la misma tarea que en las redes neuronales convencionales. Esta capa realiza la tarea de clasificación basada en las características extraídas a través de las capas anteriores y sus diferentes filtros. Mientras que las capas convolucionales y de agrupación tienden a utilizar funciones ReLu, las capas FC suelen aprovechar una función de activación softmax para clasificar adecuadamente las entradas, produciendo una probabilidad de 0 a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrimonio entre los grafos y las redes neuronales (convolucionales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los `Graph Neural Networks` (GNN) representan una de las arquitecturas que más cautivadoras y de evolución rápida dentro del panorama del aprendizaje profundo. Como modelos de aprendizdaje profundo **diseñados para procesar datos estructurados en forma de grafos**, las GNNs aportan una versatilidad notable y capacidades de aprendizaje potentes. Entre los diversos tipos de GNNs, las Redes Convolucionales de Grafos (GCNs, por sus siglas en inglés), han surgido como el modelo más prevalente y ampliamente utilizado. Estas son innovadoras puesto que permiten aprovechar tanto las características de un nodo como su localidad para hacer predicciones, proporcionando una manera efectiva de manejar datos estructurados en forma de grafo.\n",
    "\n",
    "En el *análisis de imágenes*, las GNN se emplean para modelar relaciones espaciales y semánticas entre los elementos de una imagen representados como un grafo. Estas redes son incluso capaces de capturar la estructura jerárquica e interacciones entre las regiones de la imagen, lo que mejora el rendimiento en tareas como segmentación semántica o la detección de objetos.\n",
    "\n",
    "**FUNCIONAMIENTO BÁSICO DE LAS GNN (particularizamos fórmulas a GCN)**\n",
    "\n",
    "El funcionamiento básico se basa en el concepto de \"propagación de mensajes\". En cada capa de la red se hará un cálculo de una representación para cada nodo del grafo, combinando la información que otorgan sus vecinos. Esto se logra a partir de una combinación lineal de las representaciones de estos y una función de activación no lineal. Matemáticamente, esto puede ponerse como:\n",
    "\n",
    "$$h_i = \\sum_{j \\in \\mathcal{N}_i} \\textbf{W} x_j$$\n",
    "\n",
    "donde $h_i, x_j$ son `vectores de representación de los nodos`*, **W** es la matriz de pesos (única y compartida para cada nodo).\n",
    "\n",
    "Estos vectores de representación de los nodos se eligen dependiendo de la aplicación: pueden ser *word vectors* si trabajamos con bases de conocimiento, *valores de píxeles* si trabajamos con imágenes...\n",
    "\n",
    "Estrictamente, podemos extender esta notación para que nos sirva si hay capas sucesivas, añadiendo también la función de activación explícitamente ya.\n",
    "\n",
    "$$h_{i}^{l + 1} = f\\left(\\sum_{j \\in \\mathcal{N}_i} \\frac{1}{c_{ij}} \\textbf{W} h_{j}^{l}\\right)$$\n",
    "\n",
    "donde $c_{ij}$ es una normalización que se implementa para asegurar que tenemos un rango similar de valores para todos los nodos (ya que habrá nodos con un montón de vecinos y otros más solos que la una). Normalmente, se toma $c_{ij} = \\sqrt{\\textrm{deg}(i)} \\sqrt{\\textrm{deg}(j)}$, siendo $\\textrm{deg}()$ la función que cuenta el número de conexiones en un nodo.\n",
    "\n",
    "De forma matricial la cosa suele ser más elegante, no siendo una excepción esta vez:\n",
    "\n",
    "$$H^{l + 1} = f(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{l} \\textbf{W}^{l})$$\n",
    "\n",
    "donde $H$ es la matriz con las representaciones de los nodos, $X$ es la matriz de características de los nodos, $\\hat{A}$ es la matriz de adyacencia, $\\hat{D}$ es la matriz *degree* del grafo y $W$ es la matriz de pesos entrenables.\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/adjacency_matrix.png\" alt=\"Matriz de adyacencia\">\n",
    "  <figcaption>Figura 9: Matriz de adyacencia, esquema para entender el concepto.</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Nota**: En los grafos del mundo real, la mayoría de nodos están conectados solo con unos cuantos, resultando esto en un número enorme de ceros en la matriz de adyacencia. Almacenar tantos ceros simplemente no es eficiente, por lo que se utilizan otros métodos como el de **coordinate list** (COO), que permite almacenar solo elementos distintos de 0.\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"./data/coo.gif\" alt=\"COO format\">\n",
    "  <figcaption>Figura 10: COO format.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meter aquí ya cosas de los papers vistos y el bla bla bla de cómo está siendo muy importante en la detección de anomalías esto. Mejor meter el ejemplo de towardsdatascience con alguna cosita más que pueda encontrar de fórmulas o algo así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación en detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencionar funcionamiento de algoritmo DOMINANT de manera básica?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
